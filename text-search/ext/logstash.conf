# read file contents
input {
  file {
    # TODO: fill in the absolute path to the file, or use a relative path to Logstash's home directory
    # path => "sample/msmarco-docs.tsv"

    # read the file from the beginning
    start_position => "beginning"
    # forget where we left off. This way, we re-feed everything every time we start Logstash.
    sincedb_path => "/dev/null"
  }
}

# parse every line into the fields in our schema
filter {
  # a line looks like this:
  # D1098819        https://en.wikipedia.org/wiki/Aspen,_Colorado   Aspen, Colorado "Aspen, ColoradoFrom Wikipedia, the ""free"" encyclopedia"
  # note the escaped quotes in the body

  # first, we parse the line into fields by using a grok filter. A CSV filter would have worked too, but the escaped quotes
  # break the CSV parser.
  grok {
    match => { "message" => "%{DATA:id}\t%{DATA:url}\t%{DATA:title}\t%{GREEDYDATA:body}" }
  }

  # remove fields we don't need (if we parsed the line successfully)
  if "_grokparsefailure" not in [tags] {
    mutate {
      remove_field => ["@timestamp", "@version", "event", "host", "log", "message"]
      # remove escaped quotes from the body
      gsub => ["body", '""', '"']
    }
  }
}

# write the resulting JSON documents
output {
  # write to stdout for debugging if we failed to parse the line
  if "_grokparsefailure" in [tags] {
    stdout {}
  } else {
    # otherwise, write to Vespa
    vespa_feed {
      vespa_url => "http://localhost:8080"
      namespace => "msmarco"
      document_type => "msmarco"
    }
  }
}