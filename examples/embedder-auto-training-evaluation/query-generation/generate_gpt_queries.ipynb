{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# LLM Query Synthesis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup\n",
    "\n",
    "Before we can generate queries, we need to do some setup.\n",
    "Make sure to set the following variables."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset_name = \"fiqa\" # fiqa, nq, nfcorpus\n",
    "is_cloud_deployment = False # False if using Docker\n",
    "\n",
    "# NB: Set if using Vespa Cloud\n",
    "tenant = \"my-tenant\"\n",
    "app = \"my-app\"\n",
    "instance = \"default\"\n",
    "url = \"my-endpoint-url\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Make sure a \".env\" file exists containing your OpenAI API key."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import openai\n",
    "from pathlib import Path\n",
    "\n",
    "openai.api_key_path = \"./.env\"\n",
    "root = Path(\"..\")\n",
    "dataset = root / \"datasets\" / dataset_name\n",
    "\n",
    "# Many datasets are divided into train, val/dev and test splits, while some are not.\n",
    "# This information is needed when generating qrels, among other things.\n",
    "has_splits = any(dataset.glob('train-*'))\n",
    "queries_file_name = \"train-queries\" if has_splits else \"queries\"\n",
    "qrels_file_name = \"train-qrels\" if has_splits else \"qrels\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "While we don't need Vespa to generate the queries, we will be using Vespa to generate [qrels](https://trec.nist.gov/data/qrels_eng/).\n",
    "Run this cell to connect to your Vespa app (assuming it's running)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from vespa.application import  Vespa\n",
    "\n",
    "if is_cloud_deployment:\n",
    "    vespa_dir = Path.home() / \".vespa\" / f\"{tenant}.{app}.{instance}\"\n",
    "    vespa_app = Vespa(\n",
    "        url=url,\n",
    "        cert=vespa_dir / \"data-plane-public-cert.pem\",\n",
    "        key=vespa_dir / \"data-plane-private-key.pem\"\n",
    "    )\n",
    "else:\n",
    "    # Docker deployment\n",
    "    vespa_app = Vespa(url = \"http://localhost:8080\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Loading the documents into a dictionary makes them easier to work with"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "import json\n",
    "\n",
    "def load_docs_as_dict(docs_path: Path) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    :param docs_path:\n",
    "    :return: A dictionary mapping document ids to document text\n",
    "    \"\"\"\n",
    "    docs = {}\n",
    "    with open(docs_path) as d:\n",
    "        for line in d:\n",
    "            json_line = json.loads(line)\n",
    "            did = json_line[\"doc_id\"]\n",
    "            passage = json_line[\"text\"]\n",
    "            title = json_line[\"title\"]\n",
    "            docs[did] = str({\"title\": title, \"passage\": passage})\n",
    "\n",
    "    return docs\n",
    "\n",
    "documents = load_docs_as_dict(dataset / \"docs.jsonl\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In many cases, it's useful to be able to have a mapping between queries and documents, based on a line in a qrels file.\n",
    "This class can be used to load information about the query and document connected by a given qrel."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class QueryDocumentMapping:\n",
    "    \"\"\"Usage: mapping = QueryDocumentMapping.from_qrel(qrel_file, line)\"\"\"\n",
    "\n",
    "    qid: str = None\n",
    "    query_text: str = None\n",
    "    did: str = None\n",
    "    document_text: str = None\n",
    "\n",
    "    @staticmethod\n",
    "    def from_qrel(query_file: Path, qrel: str) -> \"QueryDocumentMapping\":\n",
    "        qid, _, did, _ = qrel.split(\" \")\n",
    "\n",
    "        query_text = None\n",
    "\n",
    "        # Expand query text\n",
    "        with open(query_file) as qf:\n",
    "            for line in qf:\n",
    "                query_id, text = line.split(\"\\t\")\n",
    "                if query_id == qid:\n",
    "                    query_text = text\n",
    "                    break\n",
    "        if not query_text:\n",
    "            raise ValueError(f\"No query found for id {qid}\")\n",
    "\n",
    "        # Expand document text\n",
    "        document_text = documents[did]\n",
    "\n",
    "        return QueryDocumentMapping(qid=qid, query_text=query_text.rstrip(), did=did, document_text=document_text)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import List\n",
    "\n",
    "def get_random_doc_ids(num: int) -> List[int] | None:\n",
    "    \"\"\"\n",
    "    :param num: number of doc ids to fetch\n",
    "    :return: Returns a random list of document ids.\n",
    "    \"\"\"\n",
    "\n",
    "    list_of_docs = list(documents)\n",
    "    if len(list_of_docs) >= num:\n",
    "        return random.sample(list_of_docs, num)\n",
    "    else:\n",
    "        print(f\"Documents dictionary has less than {num} documents. Can't sample documents.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_random_lines(file: Path, num_lines: int) -> List:\n",
    "    \"\"\"\n",
    "    :param file: The file to get lines from\n",
    "    :param num_lines: How many lines to fetch\n",
    "    :return: The list of randomly selected lines\n",
    "    \"\"\"\n",
    "    with open(file, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    indices = random.sample(range(len(lines)), num_lines)\n",
    "    sample = [lines[i] for i in indices]\n",
    "\n",
    "    return sample"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_random_qrels(qrel_file: Path, query_file: Path, num_qrels: int) -> List[QueryDocumentMapping]:\n",
    "    \"\"\"\n",
    "    :param qrel_file: File to sample qrel lines from\n",
    "    :param num_qrels: How many qrels to return\n",
    "    :return: List of qrels\n",
    "    \"\"\"\n",
    "    lines = get_random_lines(qrel_file, num_qrels)\n",
    "    return [QueryDocumentMapping.from_qrel(query_file, line) for line in lines]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ChatGPT stuff"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Generating random examples\n",
    "\n",
    "Instead of creating a detailed prompt outlining exactly how queries should be generated,\n",
    "we can show ChatGPT some examples of associations between queries and documents.\n",
    "This is called few-shot learning and appears to improve the quality of the generated queries.\n",
    "Randomly sampling examples from the existing training data should in theory\n",
    "prevent the generated queries from becoming too similar to each other."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "\n",
    "def generate_random_examples(num_examples: int=1, queries_per_ex_doc: int=1) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Create a list of examples of documents and queries that return those documents.\n",
    "    :param num_examples: How many document-query pairs to generate\n",
    "    :param queries_per_ex_doc: How many queries should be shown per example document\n",
    "    :return: List of dictionary mapping document texts to query text(s)\n",
    "    \"\"\"\n",
    "    mappings = get_random_qrels(dataset / qrels_file_name, dataset / queries_file_name, num_examples)\n",
    "    if queries_per_ex_doc == 1:\n",
    "        return _generate_single_examples(mappings)\n",
    "    else:\n",
    "        return _generate_multiple_examples(mappings, queries_per_ex_doc)\n",
    "\n",
    "def _generate_single_examples(mappings: List[QueryDocumentMapping]) -> List[Dict[str, str]]:\n",
    "    examples = []\n",
    "    for m in mappings:\n",
    "        examples.append({\"role\": \"user\", \"content\": m.document_text})\n",
    "        examples.append({\"role\": \"assistant\", \"content\": f\"['{m.query_text}']\"})\n",
    "    return examples\n",
    "\n",
    "def _generate_multiple_examples(mappings: List[QueryDocumentMapping], examples_per_doc: int)-> List[Dict[str, str]]:\n",
    "    with open(dataset / qrels_file_name, \"r\") as qrf:\n",
    "        qrel_lines = qrf.readlines()\n",
    "\n",
    "    qrels_for_each_doc = []\n",
    "    doc_ids = [m.did for m in mappings]\n",
    "    failure_count = 0\n",
    "    failure_max = 10\n",
    "    for did in doc_ids:\n",
    "        qrels_for_current_doc = [line for line in qrel_lines if did in line]\n",
    "        if len(qrels_for_current_doc) < examples_per_doc:\n",
    "            # Means it could not find enough queries for that doc. Will attempt with a different doc.\n",
    "            failure_count += 1\n",
    "            doc_ids.extend(get_random_doc_ids(1))\n",
    "            if failure_count == failure_max:\n",
    "                raise ValueError(f\"Couldn't find {examples_per_doc} queries for all documents. Dataset probably has too few queries per document in qrels.\")\n",
    "            else:\n",
    "                continue\n",
    "        else:\n",
    "            qrels_for_each_doc.append(qrels_for_current_doc[:examples_per_doc])\n",
    "\n",
    "    doc_query_mappings = []\n",
    "    for did, qrels in zip(doc_ids, qrels_for_each_doc):\n",
    "        doc_query_mappings.append({did: [QueryDocumentMapping.from_qrel(dataset / queries_file_name, qrel).query_text for qrel in qrels]})\n",
    "\n",
    "    examples = []\n",
    "    for m in doc_query_mappings:\n",
    "        for did, queries in m.items():\n",
    "            examples.append({\"role\": \"user\", \"content\": f\"{documents[did]}\"})\n",
    "            examples.append({\"role\": \"assistant\", \"content\": f\"{str(queries)}\"})\n",
    "\n",
    "    return examples"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Some useful helper functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Calls to the OpenAI ChatCompletion API may fail for various reasons.\n",
    "**tenacity** lets us retry API calls using exponential backoff."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential,\n",
    ")\n",
    "\n",
    "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
    "def completion_with_backoff(**kwargs):\n",
    "    return openai.ChatCompletion.create(**kwargs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from openai import ChatCompletion\n",
    "\n",
    "def get_chat_completion_content(completion: ChatCompletion) -> str:\n",
    "    \"\"\"\n",
    "    Process and return the content of a ChatGPT completion as a string.\n",
    "    \"\"\"\n",
    "    comp_dict = completion.to_dict_recursive()\n",
    "    return comp_dict[\"choices\"][0][\"message\"][\"content\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Qrel generation\n",
    "\n",
    "Query relevance judgements (qrels) are used to determine if a given document is relevant for a given query.\n",
    "These are normally made manually by humans following certain guidelines.\n",
    "It would be beneficial to be able to automatically generate these qrels using ChatGPT.\n",
    "\n",
    "For each generated query, we'll use Vespa to return a list of documents.\n",
    "Then, we'll ask ChatGPT to determine whether each returned document is relevant."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def vespa_query(app, query, num_hits=3):\n",
    "    body = {\n",
    "        \"yql\": \"select * from doc where ({targetHits:100}nearestNeighbor(embedding,e))\",\n",
    "        \"input.query(e)\": f\"embed({query})\",\n",
    "        \"hits\": num_hits,\n",
    "        \"ranking\": \"ann\",\n",
    "    }\n",
    "    return app.query(body=body)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def process_returned_vespa_docs(query_results):\n",
    "    \"\"\"Turn the documents returned by Vespa into a less messy format.\"\"\"\n",
    "    processed_docs = []\n",
    "    for hit in query_results.hits:\n",
    "        id = hit[\"id\"].split(\"::\")[-1]\n",
    "        text = hit[\"fields\"][\"abstract\"].replace(\"passage: \", \"\")\n",
    "        if \"title\" in hit[\"fields\"].keys():\n",
    "            title = hit[\"fields\"][\"title\"].replace(\"passage: \", \"\")\n",
    "            doc = {\n",
    "                \"id\": id,\n",
    "                \"title\": title,\n",
    "                \"text\": text\n",
    "            }\n",
    "        else:\n",
    "            doc = {\n",
    "                \"id\": id,\n",
    "                \"text\": text\n",
    "            }\n",
    "\n",
    "        processed_docs.append(json.dumps(doc))\n",
    "\n",
    "    return processed_docs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def split_into_chunks(lst, chunk_size):\n",
    "    \"\"\"Splits a list into chunks of the specified size.\"\"\"\n",
    "    return [lst[i:i+chunk_size] for i in range(0, len(lst), chunk_size)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We need to tell ChatGPT how it should process the list of documents returned by Vespa.\n",
    "We ended up with this prompt through trial and error.\n",
    "We found that the easiest format to receive the qrels in was as JSON.\n",
    "This sometimes fails, though, but in that case, the results are simply discarded."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "qrel_prompt = \"\"\"\n",
    "You are an advanced relevance ranking system.\n",
    "You will receive a query and a list of documents, and output whether each document is relevant to the query.\n",
    "A document is relevant if it, in a perfect world, is supposed to show up in the retrieved documents list for a given search.\n",
    "Relevant documents are denoted with the number 1, and non-relevant documents are denoted with the number 0.\n",
    "\n",
    "### Output format ###\n",
    "The output is a valid list of JSON objects with the following fields\n",
    "\n",
    "[\n",
    "    {\n",
    "     \"id\": <id>,\n",
    "     \"relevant\": <1 or 0>\n",
    "    },\n",
    "    {\n",
    "     \"id\": <id>,\n",
    "     \"relevant\": <1 or 0>\n",
    "    }\n",
    "]\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def generate_qrels(qid: int, query: str, qrels_per_query: int, qrels_file: Path) -> List[str]:\n",
    "    \"\"\"\n",
    "    Generate (up to) a given number of qrels for a query and write to file.\n",
    "    If the number of relevant documents retrieved by Vespa is lower than qrels_per_query,\n",
    "    then the number of qrels generated will be a bit lower.\n",
    "\n",
    "    :param qid: Query id\n",
    "    :param query: Query text to generate qrels from\n",
    "    :param qrels_per_query: Target number of qrels to generate for 'query'\n",
    "    :return: The generated qrels\n",
    "    \"\"\"\n",
    "\n",
    "    result = vespa_query(vespa_app, query, num_hits=qrels_per_query)\n",
    "    docs = process_returned_vespa_docs(result)\n",
    "\n",
    "    chunked_docs = split_into_chunks(docs, 5) # Processing in chunks to save money by using 4k context instead of 16k\n",
    "\n",
    "    qrels = []\n",
    "    for chunk in chunked_docs:\n",
    "        chat_completion = completion_with_backoff(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": qrel_prompt},\n",
    "                {\"role\": \"user\", \"content\": f\"Query: {query}\\nDocuments: \" + str(chunk)},\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        completion_content = get_chat_completion_content(chat_completion)\n",
    "        with open(qrels_file, \"a+\") as gpt_qrels_file:\n",
    "            try:\n",
    "                comp = json.loads(completion_content)\n",
    "                for doc in comp:\n",
    "                    if doc[\"relevant\"] == 1:\n",
    "                        qrel = f\"{qid} 0 {doc['id']} 1\\n\"\n",
    "                        gpt_qrels_file.write(qrel)\n",
    "                        qrels.append(qrel)\n",
    "                print(comp)\n",
    "            except:\n",
    "                print(f\"ChatGPT probably returned invalid JSON:\\n{completion_content}\")\n",
    "\n",
    "    return qrels"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prompt generation\n",
    "\n",
    "Ideally, as little manual labor as possible should be necessary to generate data.\n",
    "ChatGPT does require some guidance specific to each dataset, though.\n",
    "In order to avoid having to hand craft prompts describing each dataset,\n",
    "we've come up with a way to generate rules that describe the dataset, using ChatGPT.\n",
    "ChatGPT is shown a random selection of query-document pairs, as well as queries by themselves,\n",
    "and asked to describe the dataset with a bullet list.\n",
    "This list is later injected into the prompt used to actually generate queries.\n",
    "\n",
    "The generated rules can vary wildly between runs.\n",
    "Presumably, some generated prompts will be better than others.\n",
    "As of right now, we don't have a systematic way of determining which prompt is best.\n",
    "Instead, we simply take a good look at the prompt to decide if it seems sound."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_query_doc_pairs = 10 # These numbers may be tweaked to show ChatGPT more or less data\n",
    "num_ex_queries = 15\n",
    "\n",
    "ex_qrels = get_random_qrels(dataset / qrels_file_name, dataset / queries_file_name, num_query_doc_pairs)\n",
    "ex_queries = [line.split(\"\\t\")[1] for line in get_random_lines(dataset / queries_file_name, num_ex_queries)]\n",
    "examples = \"Example query-document pairs:\\n\"\n",
    "for qrel in ex_qrels:\n",
    "    examples += \"Document: \" + json.dumps(qrel.document_text, indent=4)\n",
    "    examples += \"\\nQuery: \" + qrel.query_text + \"\\n\\n\"\n",
    "\n",
    "examples += \"Example queries:\\n\"\n",
    "for query in ex_queries:\n",
    "    examples += query"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "autogen_prompt = f\"\"\"\n",
    "You are an AI system designed to describe datasets.\n",
    "You will provide rules that describe how the queries of the dataset are formulated by looking at query-document pairs as well as queries.\n",
    "Please include information about the length, style and formatting of the queries, among other things.\n",
    "Focus on the style of the _query_ in the examples provided.\n",
    "\n",
    "Please output a list of rules that describe the dataset as a bullet list like this:\n",
    "- Queries contain x-y words\n",
    "- Queries are [terse, elaborate, etc.]\n",
    "- ...\n",
    "\"\"\"\n",
    "\n",
    "def autogenerate_prompt():\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": autogen_prompt},\n",
    "        {\"role\": \"user\", \"content\": examples}\n",
    "    ]\n",
    "\n",
    "    chat_completion = completion_with_backoff(\n",
    "        model=\"gpt-3.5-turbo-16k\",\n",
    "        messages=messages\n",
    "    )\n",
    "\n",
    "    return get_chat_completion_content(chat_completion)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rules = autogenerate_prompt()\n",
    "print(rules)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Query generation\n",
    "\n",
    "This is the cool part.\n",
    "To generate queries, ChatGPT is first shown a couple of examples of query-document pairs.\n",
    "Then, it's shown a new document and asked to generate a one or more queries that should return this document."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Some setup"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "An experiment consists of a batch of generated queries and qrels, as well as information related to the experiment (like the prompt used, number of examples, etc.)\n",
    "Experiments are saved to the **experiments** directory."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def generate_new_experiment_dir() -> str:\n",
    "    now = datetime.now()\n",
    "    timestamp = now.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    exp_dir = Path.cwd().parent / \"experiments\" / dataset_name / timestamp\n",
    "    exp_dir.mkdir(parents=True, exist_ok=True)\n",
    "    return exp_dir"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "It can be useful to be able to load a previous experiment to do more work on it, for example if query generation crashes.\n",
    "Enable the flag below to load the last experiment, or set the **experiment_dir** variable manually to load an arbitrary experiment.\n",
    "If you're running this for the first time, though, you most likely want to ignore this."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "load_previous_experiment = False\n",
    "\n",
    "def previous_experiment() -> Path:\n",
    "    parent_dir = Path.cwd().parent / \"experiments\" / dataset_name\n",
    "    dirs = [f for f in parent_dir.iterdir() if f.is_dir()]\n",
    "    return dirs[0]\n",
    "\n",
    "if load_previous_experiment:\n",
    "    experiment_dir = previous_experiment()\n",
    "    print(f\"Loaded previous experiment: {experiment_dir}\")\n",
    "else:\n",
    "    experiment_dir = generate_new_experiment_dir()\n",
    "    print(f\"Created experiment: {experiment_dir}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following options determine how many queries and qrels to generate,\n",
    "as well as how many queries to generate per document."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "target_queries = 100 # How many queries to end up with in total\n",
    "queries_per_doc = 1 # Number of queries to generate per document\n",
    "num_qrels_per_query = 1 # Number of qrels per document\n",
    "\n",
    "num_queries = int(target_queries / queries_per_doc)  # How many calls to chatgpt we need to get to approx. that number"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "These options determine how many examples to show to ChatGPT for each query generation.\n",
    "We believe a higher number of examples results in higher quality queries, but this costs more money.\n",
    "In addition, if too many examples are shown, we may run out of tokens in the context window.\n",
    "Setting **examples_per_doc** too high may cause example generation to fail if the original dataset\n",
    "does not contain enough examples of documents having multiple queries."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_examples = 3 # Number of example documents shown\n",
    "examples_per_doc = 1 # Number of queries shown per example document"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "You are an AI system designed to write natural language search queries.\n",
    "You will receive a document and write {queries_per_doc} {\"queries\" if queries_per_doc > 1 else \"query\"} for which this document is relevant.\n",
    "\n",
    "### Rules ###\n",
    "{rules}\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Useful information is written to a file to help document the experiment and make it reproducible."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "info_file = experiment_dir / \"info.txt\"\n",
    "info_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "info_file.touch()\n",
    "with open(info_file, \"w\") as info:\n",
    "    info.write(f\"{prompt = }\\n\")\n",
    "    info.write(f\"{num_examples = }\\n\")\n",
    "    info.write(f\"{examples_per_doc = }\\n\")\n",
    "    info.write(f\"{queries_per_doc = }\\n\")\n",
    "    info.write(f\"{target_queries = }\\n\")\n",
    "    info.write(f\"{num_queries = }\\n\")\n",
    "    info.write(f\"{num_qrels_per_query = }\\n\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To generate unique IDs for GPT-generated queries, we use the SHA256 hash function.\n",
    "The chance of a collision should be extremely low, and in case there ever is one,\n",
    "it probably wouldn't affect the rest of the process that much (famous last words)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "def get_hash_id(seed: str) -> str:\n",
    "    \"\"\"\n",
    "    :param seed: Value to be hashed. Should be unique. Could be query text.\n",
    "    :return: The new query id\n",
    "    \"\"\"\n",
    "    hash_object = hashlib.sha256(seed.encode())\n",
    "    hex_dig = hash_object.hexdigest()\n",
    "\n",
    "    return \"GPT-\" + hex_dig\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Query generation\n",
    "Run the cells below to generate queries.\n",
    "The generated queries and qrels are written to file continuously in case an error interrupts the process."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Keep track of how many queries have been generated.\n",
    "# If the query generation crashes, you can rerun the cell below to continue from where it crashed.\n",
    "generated_so_far = 0"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "# Used to keep track of cost of query generation\n",
    "prompt_tokens = []\n",
    "completion_tokens = []\n",
    "\n",
    "# Resume in case generation fails\n",
    "num_queries = int((target_queries - generated_so_far) / queries_per_doc)\n",
    "\n",
    "for idx, did in enumerate(get_random_doc_ids(num_queries)):\n",
    "    random_examples = generate_random_examples(num_examples=num_examples, queries_per_ex_doc=examples_per_doc)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": prompt},\n",
    "        *random_examples,\n",
    "        {\"role\": \"user\", \"content\": str(documents[did])},\n",
    "    ]\n",
    "\n",
    "    chat_completion = completion_with_backoff(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=messages\n",
    "    )\n",
    "\n",
    "    pt, ct, _ = chat_completion.to_dict_recursive()[\"usage\"].values()\n",
    "    prompt_tokens.append(pt)\n",
    "    completion_tokens.append(ct)\n",
    "\n",
    "    with open(experiment_dir / \"gpt-queries\", \"a\") as gpt_queries:\n",
    "        with open(experiment_dir / \"gpt-qrels\", \"a\") as gpt_qrels:\n",
    "            try:\n",
    "                queries = ast.literal_eval(get_chat_completion_content(chat_completion)) # Get list from list literal. Should be safe (security wise), but can fail\n",
    "                generated_so_far += 1\n",
    "            except (ValueError, SyntaxError):\n",
    "                print(\"Failed to parse generated queries, skipping this document.\")\n",
    "                continue  # Malformed output from ChatGPT. Skip to the next document\n",
    "            print(idx)\n",
    "            print(documents[did])\n",
    "            for query in queries:\n",
    "                qid = get_hash_id(query)\n",
    "                gpt_queries.write(f\"{qid}\\t{query}\\n\")\n",
    "                gpt_qrels.write(f\"{qid} 0 {did} 1\\n\")\n",
    "\n",
    "                print(query)\n",
    "                if num_qrels_per_query > 1:\n",
    "                    generate_qrels(qid, query, num_qrels_per_query, experiment_dir / f\"gpt-qrels\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The mean number of tokens used is kept track of and written to a file.\n",
    "This can be used to calculate the cost of query generation."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "mean_comp = np.mean(completion_tokens)\n",
    "mean_prompt = np.mean(prompt_tokens)\n",
    "print(\"Completions tokens:\",mean_comp)\n",
    "print(\"Prompt tokens:\", mean_prompt)\n",
    "\n",
    "tokens_file_path = experiment_dir / \"tokens.txt\"\n",
    "tokens_file_path.touch()\n",
    "with open(tokens_file_path, \"w\") as tokens_file:\n",
    "    tokens_file.write(f\"Completion tokens: {mean_comp}\\n\")\n",
    "    tokens_file.write(f\"Prompt tokens: {mean_prompt}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Post processing\n",
    "\n",
    "Sometimes, it's useful to generate more qrels after queries have been generated, or to remove qrels after the fact.\n",
    "The following snippets help with that.\n",
    "Don't run this unless you know what you're doing."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Generate additional qrels.\n",
    "These are written to a new file, in order to avoid cluttering the old one.\n",
    "For training, you'll need to merge the two qrel files and filter out duplicates."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gen_qrels = False # Whether to generate more qrels or not\n",
    "extra_qrels_per_query = 10 # per doc\n",
    "\n",
    "def generate_extra_qrels(num_qrels: int, resume_from: int=0):\n",
    "    \"\"\"\n",
    "    Generates more qrels for GPT-generated queries.\n",
    "    :param num_qrels: How many qrels to generate per document\n",
    "    :param resume_from: Ignores the first x documents. Useful if something crashes and you want to resume.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    with open(experiment_dir / \"extra-qrels-info.txt\", \"w\") as infof:\n",
    "        infof.write(f\"{num_qrels = }\\n\")\n",
    "\n",
    "    qrel_in_file = experiment_dir / \"gpt-qrels\" # Existing qrels from inital generation\n",
    "    qrel_out_file = experiment_dir / \"gpt-qrels-extra\"\n",
    "    qrel_out_file.touch()\n",
    "    with open(qrel_in_file, \"r\") as qrel_file:\n",
    "        for i, qrel in enumerate(qrel_file.readlines()):\n",
    "            if i < resume_from:\n",
    "                continue\n",
    "            print(i)\n",
    "            mapping = QueryDocumentMapping.from_qrel(experiment_dir / \"gpt-queries\", qrel)\n",
    "            generate_qrels(mapping.qid, mapping.query_text, qrels_per_query=num_qrels, qrels_file=qrel_out_file)\n",
    "\n",
    "if gen_qrels:\n",
    "    generate_extra_qrels(extra_qrels_per_query)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "When generating a query, a qrel is automatically created.\n",
    "In addition to this singular qrel, additional qrels can be generated with **generate_qrels()**.\n",
    "This snippet removes all qrels except the one that was originally generated alongside with the query.\n",
    "This can be useful for benchmarking, in case you want to see how the embedder would perform if you have fewer qrels.\n",
    "The filtered qrels are written to a new file, so as not to mess with the original data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "do_filter_qrels = False\n",
    "\n",
    "def filter_qrels(qrel_in_file, qrel_out_file):\n",
    "    \"\"\"Keep only the first qrel for each query.\"\"\"\n",
    "    seen = set() # Query ids for queries whose first qrel has been found\n",
    "    with open(qrel_in_file, \"r\") as source_file:\n",
    "        with open(qrel_out_file, \"w\") as output_file:\n",
    "            for line in source_file:\n",
    "                query_id = line.split()[0]\n",
    "                if query_id not in seen:\n",
    "                    print(f\"{line = }\")\n",
    "                    output_file.write(line)\n",
    "                    seen.add(query_id)\n",
    "\n",
    "if do_filter_qrels:\n",
    "    filter_qrels(experiment_dir / \"gpt-qrels\", experiment_dir / \"gpt-one-qrel-per-query\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
