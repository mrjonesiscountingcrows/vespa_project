<?xml version="1.0" encoding="utf-8" ?>
<!-- Copyright Vespa.ai. Licensed under the terms of the Apache 2.0 license. See LICENSE in the project root. -->
<services version="1.0" xmlns:deploy="vespa" xmlns:preprocess="properties">

  <admin version="2.0">
    <adminserver hostalias="node1" />
  </admin>

  <!--
      A container cluster handles incoming requests to the application and processes those requests,
      and their results. The processing to do and the APIs to expose can be provided by Vespa
      or by the application through Java components supplied as part of the application.

      See:
        - Reference: https://docs.vespa.ai/en/reference/services-container.html
  -->
  <container id="default" version="1.0">
    <!--
        <document-api> tells the container that it should accept documents for indexing. Through the
        Document REST API you can PUT new documents, UPDATE existing documents, and DELETE documents
        already in the cluster.

        Documents sent to the Document REST API will be passed through document processors on the way
        to the content cluster.

        See:
         - Reference: https://docs.vespa.ai/en/reference/services-container.html#document-api
         - Operations: https://docs.vespa.ai/en/document-v1-api-guide.html
    -->
    <document-api/>

    <!--
      TODO Explain handlers and components
    -->
    <handler id="ai.vespa.example.embedding_service.EmbeddingHandler" bundle="embedding-service">
      <binding>http://*/embedding</binding>
    </handler>

    <!-- Embedders fetched from https://cloud.vespa.ai/en/model-hub -->

    <!-- Available for local deployments and in Vespa Cloud
    -->
    <component id="e5-small-v2" type="hugging-face-embedder">
      <transformer-model model-id="e5-small-v2" path="embedder-models/e5-small-v2/model.onnx"/>
      <tokenizer-model path="embedder-models/e5-small-v2/tokenizer.json"/>
    </component>

    <!--
    The models below are only available in Vespa Cloud, unless you download the models locally (see instructions in README).
    You can uncomment the blocks to enable the models, but please be mindful of the increased memory usage.
    A brief test indicates that *at least* 12Gb of memory is consumed when all the models are enabled.
    -->

    <!--
    <component id="e5-base-v2" type="hugging-face-embedder">
      <transformer-model model-id="e5-base-v2"/>
    </component>
    -->

    <!--
    <component id="e5-large-v2" type="hugging-face-embedder">
      <transformer-model model-id="e5-large-v2"/>
    </component>
    -->

    <!--
    <component id="multilingual-e5-base" type="hugging-face-embedder">
      <transformer-model model-id="multilingual-e5-base"/>
    </component>
    -->

    <!--
    <component id="minilm-l6-v2" type="bert-embedder">
      <transformer-model model-id="minilm-l6-v2" />
      <tokenizer-vocab model-id="bert-base-uncased"/>
    </component>
    -->

    <!--
        <nodes> specifies the nodes that should run this cluster, and through the <resources>
        subtag, the resources on those nodes. You can also specify ranges of nodes and resources
        to activate autoscaling.

        See:
         - Reference: https://docs.vespa.ai/en/reference/services.html
    -->
    <nodes>
      <node hostalias="node1"/>

      <!-- Example of configuring more memory for larger models, using Vespa Cloud -->
      <resources vcpu="4.0" memory="16Gb"/>
    </nodes>
  </container>

</services>
